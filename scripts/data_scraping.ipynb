{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f8e69a",
   "metadata": {},
   "source": [
    "# Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62573ac9",
   "metadata": {},
   "source": [
    "Dies ist die Analyse der Daten des CDC f√ºr Health and Nutrition aus den USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576d92e",
   "metadata": {},
   "source": [
    "2013-14: https://www.kaggle.com/datasets/cdc/national-health-and-nutrition-examination-survey \\\n",
    "2017-18: https://www.kaggle.com/datasets/rileyzurrin/national-health-and-nutrition-exam-survey-2017-2018/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a5d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e24556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\srnwn\\\\Documents\\\\neueFische\\\\Capstone\\\\scripts'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2358161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sas(\"../data/2024/Bodymeasures.xpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff85f7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>BMDSTATS</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMIWT</th>\n",
       "      <th>BMXRECUM</th>\n",
       "      <th>BMIRECUM</th>\n",
       "      <th>BMXHEAD</th>\n",
       "      <th>BMIHEAD</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMIHT</th>\n",
       "      <th>...</th>\n",
       "      <th>BMXLEG</th>\n",
       "      <th>BMILEG</th>\n",
       "      <th>BMXARML</th>\n",
       "      <th>BMIARML</th>\n",
       "      <th>BMXARMC</th>\n",
       "      <th>BMIARMC</th>\n",
       "      <th>BMXWAIST</th>\n",
       "      <th>BMIWAIST</th>\n",
       "      <th>BMXHIP</th>\n",
       "      <th>BMIHIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130378.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>42.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130382.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8855</th>\n",
       "      <td>142306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8856</th>\n",
       "      <td>142307.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8857</th>\n",
       "      <td>142308.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>41.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8858</th>\n",
       "      <td>142309.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8859</th>\n",
       "      <td>142310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8860 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN  BMDSTATS  BMXWT  BMIWT  BMXRECUM  BMIRECUM  BMXHEAD  BMIHEAD  \\\n",
       "0     130378.0       1.0   86.9    NaN       NaN       NaN      NaN      NaN   \n",
       "1     130379.0       1.0  101.8    NaN       NaN       NaN      NaN      NaN   \n",
       "2     130380.0       1.0   69.4    NaN       NaN       NaN      NaN      NaN   \n",
       "3     130381.0       1.0   34.3    NaN       NaN       NaN      NaN      NaN   \n",
       "4     130382.0       3.0   13.6    NaN       NaN       1.0      NaN      NaN   \n",
       "...        ...       ...    ...    ...       ...       ...      ...      ...   \n",
       "8855  142306.0       1.0   25.3    NaN       NaN       NaN      NaN      NaN   \n",
       "8856  142307.0       3.0    NaN    1.0       NaN       NaN      NaN      NaN   \n",
       "8857  142308.0       1.0   79.3    NaN       NaN       NaN      NaN      NaN   \n",
       "8858  142309.0       1.0   81.9    NaN       NaN       NaN      NaN      NaN   \n",
       "8859  142310.0       1.0   72.1    NaN       NaN       NaN      NaN      NaN   \n",
       "\n",
       "      BMXHT  BMIHT  ...  BMXLEG  BMILEG  BMXARML  BMIARML  BMXARMC  BMIARMC  \\\n",
       "0     179.5    NaN  ...    42.8     NaN     42.0      NaN     35.7      NaN   \n",
       "1     174.2    NaN  ...    38.5     NaN     38.7      NaN     33.7      NaN   \n",
       "2     152.9    NaN  ...    38.5     NaN     35.5      NaN     36.3      NaN   \n",
       "3     120.1    NaN  ...     NaN     NaN     25.4      NaN     23.4      NaN   \n",
       "4       NaN    1.0  ...     NaN     NaN      NaN      1.0      NaN      1.0   \n",
       "...     ...    ...  ...     ...     ...      ...      ...      ...      ...   \n",
       "8855  128.0    NaN  ...    32.0     NaN     25.0      NaN     19.0      NaN   \n",
       "8856  143.8    NaN  ...     NaN     1.0     34.0      NaN     35.4      NaN   \n",
       "8857  173.3    NaN  ...    41.8     NaN     40.0      NaN     30.6      NaN   \n",
       "8858  179.1    NaN  ...    44.0     NaN     40.0      NaN     30.8      NaN   \n",
       "8859  161.7    NaN  ...    34.2     NaN     34.0      NaN     33.5      NaN   \n",
       "\n",
       "      BMXWAIST  BMIWAIST  BMXHIP  BMIHIP  \n",
       "0         98.3       NaN   102.9     NaN  \n",
       "1        114.7       NaN   112.4     NaN  \n",
       "2         93.5       NaN    98.0     NaN  \n",
       "3         70.4       NaN     NaN     NaN  \n",
       "4          NaN       1.0     NaN     NaN  \n",
       "...        ...       ...     ...     ...  \n",
       "8855      57.7       NaN     NaN     NaN  \n",
       "8856       NaN       1.0     NaN     1.0  \n",
       "8857      98.4       NaN    97.7     NaN  \n",
       "8858      96.0       NaN   103.3     NaN  \n",
       "8859     110.8       NaN   115.0     NaN  \n",
       "\n",
       "[8860 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713898ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge duplicate columns. \n",
    "def combine_dupes(df: pd.DataFrame):\n",
    "\n",
    "    # Begin by enumerating duplicates:\n",
    "    col = df.columns.to_list()\n",
    "\n",
    "    # Initialize a dictionary to store indices\n",
    "    index_dict = {}\n",
    "\n",
    "    # Iterate over the array and store indices for each unique value\n",
    "    for index, value in enumerate(col):\n",
    "        if value not in index_dict:\n",
    "            index_dict[value] = [index]\n",
    "        else:\n",
    "            index_dict[value].append(index)\n",
    "\n",
    "    # Filter out values with only one occurrence (non-duplicates)\n",
    "    duplicates_dict = {key: value for key, value in index_dict.items() if len(value) > 1}\n",
    "\n",
    "    # Merge duplicates and enumerate columns that need to be dropped\n",
    "    col2drop = []\n",
    "    for _, value in duplicates_dict.items():\n",
    "        for col in value[1:]:\n",
    "            df.iloc[:, value[0]] = df.iloc[:, value[0]].combine_first(df.iloc[:, col])\n",
    "            col2drop.append(col)\n",
    "\n",
    "    # Drop all but first duplicate columns\n",
    "    df = df.iloc[:, [j for j, c in enumerate(df.columns) if j not in col2drop]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a1e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Now working on demographics file\n",
      "INFO:root:Extracting DEMO_D.xpt\n",
      "INFO:root:Extracting DEMO_E.xpt\n",
      "INFO:root:Extracting DEMO_C.xpt\n",
      "INFO:root:Extracting DEMO_B.xpt\n",
      "INFO:root:Extracting DEMO.xpt\n",
      "INFO:root:Extracting DEMO_F.xpt\n",
      "INFO:root:Extracting DEMO_G.xpt\n",
      "INFO:root:Extracting DEMO_H.xpt\n",
      "INFO:root:Extracting DEMO_I.xpt\n",
      "INFO:root:Extracting DEMO_J.xpt\n",
      "INFO:root:Extracting P_DEMO.xpt\n",
      "INFO:root:Extracting DEMO_L.xpt\n",
      "INFO:root:Finished and saved demographics file\n",
      "INFO:root:Now working on dietary file\n",
      "INFO:root:Extracting DRXIFF.xpt\n",
      "INFO:root:DRXIFF.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DRXIFF_B.xpt\n",
      "INFO:root:DRXIFF_B.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DR1IFF_C.xpt\n",
      "INFO:root:DR1IFF_C.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_F.xpt\n",
      "INFO:root:DR1IFF_F.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_E.xpt\n",
      "INFO:root:DR1IFF_E.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_G.xpt\n",
      "INFO:root:DR1IFF_G.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_D.xpt\n",
      "INFO:root:DR1IFF_D.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_H.xpt\n",
      "INFO:root:DR1IFF_H.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_I.xpt\n",
      "INFO:root:DR1IFF_I.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_J.xpt\n",
      "INFO:root:DR1IFF_J.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting P_DR1IFF.xpt\n",
      "INFO:root:P_DR1IFF.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR1IFF_L.xpt\n",
      "INFO:root:DR1IFF_L.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_C.xpt\n",
      "INFO:root:DR2IFF_C.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_F.xpt\n",
      "INFO:root:DR2IFF_F.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_E.xpt\n",
      "INFO:root:DR2IFF_E.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_G.xpt\n",
      "INFO:root:DR2IFF_G.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_D.xpt\n",
      "INFO:root:DR2IFF_D.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_H.xpt\n",
      "INFO:root:DR2IFF_H.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_I.xpt\n",
      "INFO:root:DR2IFF_I.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_J.xpt\n",
      "INFO:root:DR2IFF_J.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting P_DR2IFF.xpt\n",
      "INFO:root:P_DR2IFF.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DR2IFF_L.xpt\n",
      "INFO:root:DR2IFF_L.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DRXTOT.xpt\n",
      "INFO:root:Extracting DRXTOT_B.xpt\n",
      "INFO:root:Extracting DR1TOT_C.xpt\n",
      "INFO:root:Extracting DR1TOT_F.xpt\n",
      "INFO:root:Extracting DR1TOT_E.xpt\n",
      "INFO:root:Extracting DR1TOT_G.xpt\n",
      "INFO:root:Extracting DR1TOT_D.xpt\n",
      "INFO:root:Extracting DR1TOT_H.xpt\n",
      "INFO:root:Extracting DR1TOT_I.xpt\n",
      "INFO:root:Extracting DR1TOT_J.xpt\n",
      "INFO:root:Extracting P_DR1TOT.xpt\n",
      "INFO:root:Extracting DR1TOT_L.xpt\n",
      "INFO:root:Extracting DR2TOT_C.xpt\n",
      "INFO:root:Extracting DR2TOT_F.xpt\n",
      "INFO:root:Extracting DR2TOT_E.xpt\n",
      "INFO:root:Extracting DR2TOT_G.xpt\n",
      "INFO:root:Extracting DR2TOT_D.xpt\n",
      "INFO:root:Extracting DR2TOT_H.xpt\n",
      "INFO:root:Extracting DR2TOT_I.xpt\n",
      "INFO:root:Extracting DR2TOT_J.xpt\n",
      "INFO:root:Extracting P_DR2TOT.xpt\n",
      "INFO:root:Extracting DR2TOT_L.xpt\n",
      "INFO:root:Extracting DRXFMT.xpt\n",
      "INFO:root:DRXFMT.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFMT_B.xpt\n",
      "INFO:root:DRXFMT_B.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_C.xpt\n",
      "INFO:root:DRXFCD_C.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_F.xpt\n",
      "INFO:root:DRXFCD_F.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_E.xpt\n",
      "INFO:root:DRXFCD_E.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_G.xpt\n",
      "INFO:root:DRXFCD_G.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_D.xpt\n",
      "INFO:root:DRXFCD_D.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_H.xpt\n",
      "INFO:root:DRXFCD_H.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_I.xpt\n",
      "INFO:root:DRXFCD_I.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_J.xpt\n",
      "INFO:root:DRXFCD_J.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting P_DRXFCD.xpt\n",
      "INFO:root:P_DRXFCD.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXFCD_L.xpt\n",
      "INFO:root:DRXFCD_L.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXMCD_F.xpt\n",
      "INFO:root:DRXMCD_F.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXMCD_E.xpt\n",
      "INFO:root:DRXMCD_E.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXMCD_G.xpt\n",
      "INFO:root:DRXMCD_G.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXMCD_D.xpt\n",
      "INFO:root:DRXMCD_D.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DRXMCD_C.xpt\n",
      "INFO:root:DRXMCD_C.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DTQ_F.xpt\n",
      "INFO:root:Extracting DSBI.xpt\n",
      "INFO:root:DSBI.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DSBI.xpt\n",
      "INFO:root:DSBI.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DSII.xpt\n",
      "INFO:root:DSII.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DSII.xpt\n",
      "INFO:root:DSII.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting DSPI.xpt\n",
      "INFO:root:DSPI.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DSPI.xpt\n",
      "INFO:root:DSPI.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting DS1IDS_F.xpt\n",
      "INFO:root:DS1IDS_F.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1IDS_E.xpt\n",
      "INFO:root:DS1IDS_E.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1IDS_G.xpt\n",
      "INFO:root:DS1IDS_G.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1IDS_H.xpt\n",
      "INFO:root:DS1IDS_H.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1IDS_I.xpt\n",
      "INFO:root:DS1IDS_I.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1IDS_J.xpt\n",
      "INFO:root:DS1IDS_J.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting P_DS1IDS.xpt\n",
      "INFO:root:P_DS1IDS.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_F.xpt\n",
      "INFO:root:DS2IDS_F.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_E.xpt\n",
      "INFO:root:DS2IDS_E.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_G.xpt\n",
      "INFO:root:DS2IDS_G.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_H.xpt\n",
      "INFO:root:DS2IDS_H.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_I.xpt\n",
      "INFO:root:DS2IDS_I.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS2IDS_J.xpt\n",
      "INFO:root:DS2IDS_J.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting P_DS2IDS.xpt\n",
      "INFO:root:P_DS2IDS.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DS1TOT_F.xpt\n",
      "INFO:root:Extracting DS1TOT_E.xpt\n",
      "INFO:root:Extracting DS1TOT_G.xpt\n",
      "INFO:root:Extracting DS1TOT_H.xpt\n",
      "INFO:root:Extracting DS1TOT_I.xpt\n",
      "INFO:root:Extracting DS1TOT_J.xpt\n",
      "INFO:root:Extracting P_DS1TOT.xpt\n",
      "INFO:root:Extracting DS2TOT_F.xpt\n",
      "INFO:root:Extracting DS2TOT_E.xpt\n",
      "INFO:root:Extracting DS2TOT_G.xpt\n",
      "INFO:root:Extracting DS2TOT_H.xpt\n",
      "INFO:root:Extracting DS2TOT_I.xpt\n",
      "INFO:root:Extracting DS2TOT_J.xpt\n",
      "INFO:root:Extracting P_DS2TOT.xpt\n",
      "INFO:root:Extracting DSQIDS_F.xpt\n",
      "INFO:root:DSQIDS_F.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_E.xpt\n",
      "INFO:root:DSQIDS_E.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_G.xpt\n",
      "INFO:root:DSQIDS_G.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQFILE1.xpt\n",
      "INFO:root:Extracting DSQ1_B.xpt\n",
      "INFO:root:Extracting DSQ1_C.xpt\n",
      "INFO:root:Extracting DSQ1_D.xpt\n",
      "INFO:root:Extracting DSQFILE2.xpt\n",
      "INFO:root:DSQFILE2.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQ2_B.xpt\n",
      "INFO:root:DSQ2_B.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQ2_C.xpt\n",
      "INFO:root:DSQ2_C.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQ2_D.xpt\n",
      "INFO:root:DSQ2_D.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_H.xpt\n",
      "INFO:root:DSQIDS_H.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_I.xpt\n",
      "INFO:root:DSQIDS_I.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_J.xpt\n",
      "INFO:root:DSQIDS_J.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting P_DSQIDS.xpt\n",
      "INFO:root:P_DSQIDS.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQIDS_L.xpt\n",
      "INFO:root:DSQIDS_L.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting DSQTOT_F.xpt\n",
      "INFO:root:Extracting DSQTOT_E.xpt\n",
      "INFO:root:Extracting DSQTOT_G.xpt\n",
      "INFO:root:Extracting DSQTOT_H.xpt\n",
      "INFO:root:Extracting DSQTOT_I.xpt\n",
      "INFO:root:Extracting DSQTOT_J.xpt\n",
      "INFO:root:Extracting P_DSQTOT.xpt\n",
      "INFO:root:Extracting DSQTOT_L.xpt\n",
      "INFO:root:Extracting FOODLK_C.xpt\n",
      "INFO:root:FOODLK_C.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting FOODLK_D.xpt\n",
      "INFO:root:FOODLK_D.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting VARLK_C.xpt\n",
      "INFO:root:VARLK_C.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting VARLK_D.xpt\n",
      "INFO:root:VARLK_D.xpt skipped because it's not based on individual participants\n",
      "INFO:root:Extracting FFQDC_D.xpt\n",
      "INFO:root:FFQDC_D.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting FFQDC_C.xpt\n",
      "INFO:root:FFQDC_C.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting FFQRAW_D.xpt\n",
      "INFO:root:Extracting FFQRAW_C.xpt\n",
      "INFO:root:Finished and saved dietary file\n",
      "INFO:root:Now working on examination file\n",
      "INFO:root:Extracting ARX_F.xpt\n",
      "INFO:root:Extracting AUX1.xpt\n",
      "INFO:root:Extracting AUX_E.xpt\n",
      "INFO:root:Extracting AUX_D.xpt\n",
      "INFO:root:Extracting AUX_C.xpt\n",
      "INFO:root:Extracting AUX_B.xpt\n",
      "INFO:root:Extracting AUX_F.xpt\n",
      "INFO:root:Extracting AUX_G.xpt\n",
      "INFO:root:Extracting AUX_I.xpt\n",
      "INFO:root:Extracting AUX_J.xpt\n",
      "INFO:root:Extracting P_AUX.xpt\n",
      "INFO:root:Extracting AUXAR_F.xpt\n",
      "INFO:root:AUXAR_F.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_E.xpt\n",
      "INFO:root:AUXAR_E.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_D.xpt\n",
      "INFO:root:AUXAR_D.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR.xpt\n",
      "INFO:root:AUXAR.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_B.xpt\n",
      "INFO:root:AUXAR_B.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_C.xpt\n",
      "INFO:root:AUXAR_C.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_G.xpt\n",
      "INFO:root:AUXAR_G.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_I.xpt\n",
      "INFO:root:AUXAR_I.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXAR_J.xpt\n",
      "INFO:root:AUXAR_J.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting P_AUXAR.xpt\n",
      "INFO:root:P_AUXAR.xpt was skipped because it has duplicate id values or is too large\n",
      "INFO:root:Extracting AUXTYM.xpt\n",
      "INFO:root:Extracting AUXTYM_E.xpt\n",
      "INFO:root:Extracting AUXTYM_D.xpt\n",
      "INFO:root:Extracting AUXTYM_C.xpt\n",
      "INFO:root:Extracting AUXTYM_B.xpt\n",
      "INFO:root:Extracting AUXTYM_F.xpt\n",
      "INFO:root:Extracting AUXTYM_G.xpt\n",
      "INFO:root:Extracting AUXTYM_I.xpt\n",
      "INFO:root:Skipped 513 variables in AUXTYM_I.xpt because they are large, unhelpful sensor data\n",
      "INFO:root:AUXTYM_I.xpt skipped because it has duplicate id values\n",
      "INFO:root:Extracting AUXTYM_J.xpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mdownload_data()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 145\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    143\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_YEAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m downloader \u001b[38;5;241m=\u001b[39m NHANESDataDownloader(base_url)\n\u001b[1;32m--> 145\u001b[0m \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m, in \u001b[0;36mNHANESDataDownloader.download_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Download and convert all data directly\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datatype, data_url \u001b[38;5;129;01min\u001b[39;00m data_urls\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_and_convert_xpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36mNHANESDataDownloader.extract_and_convert_xpt\u001b[1;34m(self, url, datatype)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Grab file, and add to queue if passes processing\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     xpt_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_and_process_xpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     dframes\u001b[38;5;241m.\u001b[39mappend(xpt_df)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[3], line 102\u001b[0m, in \u001b[0;36mNHANESDataDownloader.read_and_process_xpt\u001b[1;34m(self, url, xpt, datatype)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_process_xpt\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, xpt, datatype):\n\u001b[0;32m    101\u001b[0m     xpt_url \u001b[38;5;241m=\u001b[39m urljoin(url, xpt)\n\u001b[1;32m--> 102\u001b[0m     xpt_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxpt_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(xpt)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# For lab data, drop columns ending with \"LC\". These are comment codes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\site-packages\\pandas\\io\\sas\\sasreader.py:154\u001b[0m, in \u001b[0;36mread_sas\u001b[1;34m(filepath_or_buffer, format, index, encoding, chunksize, iterator, compression)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxport\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas_xport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XportReader\n\u001b[1;32m--> 154\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mXportReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msas7bdat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas7bdat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAS7BDATReader\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\site-packages\\pandas\\io\\sas\\sas_xport.py:270\u001b[0m, in \u001b[0;36mXportReader.__init__\u001b[1;34m(self, filepath_or_buffer, index, encoding, chunksize, compression)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunksize \u001b[38;5;241m=\u001b[39m chunksize\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py:389\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[0;32m    388\u001b[0m             compression \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 389\u001b[0m         reader \u001b[38;5;241m=\u001b[39m BytesIO(\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[0;32m    391\u001b[0m         filepath_or_buffer\u001b[38;5;241m=\u001b[39mreader,\n\u001b[0;32m    392\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m         mode\u001b[38;5;241m=\u001b[39mfsspec_mode,\n\u001b[0;32m    396\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\http\\client.py:476\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\http\\client.py:626\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    624\u001b[0m s \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 626\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXAMOUNT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s), amt)\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\srnwn\\miniconda3\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# Modify this line to year of choice. E.g., 2015\n",
    "BASE_YEAR = \"August 2021\"\n",
    "\n",
    "# Modify this to return less information while running (e.g., level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class NHANESDataDownloader:\n",
    "    def __init__(self, base_url, id=\"SEQN\"):\n",
    "        self.base_url = base_url\n",
    "        self.id = id\n",
    "        self.title = None\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    def find_data_urls(self):\n",
    "        soup = self.get_soup(self.base_url)\n",
    "\n",
    "        # Find all links, then narrow down to data URLs\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Grab title (years of NHANES data) for file-naming purposes\n",
    "        self.title = soup.find('h1', href=False).text.strip().replace(\" \", \"_\")\n",
    "\n",
    "        # Grab datatypes (usually [demographics, dietary, examination, laboratory, questionnaire, limited])\n",
    "        datatypes = [re.split(r'\\n|\\t', datatype.text.strip())[-1].split(\" \")[0].lower() for datatype in links if datatype['href'].startswith(('../search'))]\n",
    "        data_urls_list = [urljoin(self.base_url, data_url['href']) for data_url in links if data_url['href'].startswith(('../search'))]\n",
    "        \n",
    "        # Zip datatype with list of links. Return result\n",
    "        data_urls = dict(zip(datatypes, data_urls_list))\n",
    "        # Delete \"limited\" file if it exists\n",
    "        data_urls.pop(\"limited\", \"\")\n",
    "\n",
    "        return data_urls\n",
    "\n",
    "    def extract_and_convert_xpt(self, url, datatype):\n",
    "\n",
    "        logging.info(f'Now working on {datatype} file')\n",
    "        \n",
    "        soup = self.get_soup(url)\n",
    "\n",
    "        # Find all links, then narrow down to XPT files\n",
    "        links = soup.find_all('a', href=True)\n",
    "        xpt_files = [link['href'] for link in links if link['href'].lower().endswith(('.xpt'))]\n",
    "        dframes = []\n",
    "\n",
    "        # Download and convert each file\n",
    "        for xpt in xpt_files:\n",
    "            logging.info(\"Extracting \" + os.path.basename(xpt))\n",
    "            try:\n",
    "                 # Ignore unimportant files\n",
    "                if self.is_unimportant_file(xpt):\n",
    "                    logging.info(f\"{os.path.basename(xpt)} was skipped because it has duplicate id values or is too large\")\n",
    "                    continue\n",
    "                # Grab file, and add to queue if passes processing\n",
    "                xpt_df = self.read_and_process_xpt(url, xpt, datatype)\n",
    "                dframes.append(xpt_df)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {os.path.basename(xpt)}: {e}\")\n",
    "\n",
    "        # Clean up data: remove duplicate columns\n",
    "        df = combine_dupes(pd.concat(dframes, axis=1))\n",
    "        # Replace values smaller than the threshold=10**-30 with 0\n",
    "        num = df._get_numeric_data()\n",
    "        num[num < 10E-30] = 0\n",
    "\n",
    "        # Drop \"SAMPLEID\" column, as it was only relevant to pooled data.\n",
    "        try:\n",
    "            df.drop(\"SAMPLEID\", axis=1)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # Extract the file name using the URL and save as CSV\n",
    "        file_name = os.path.join(self.title, datatype)\n",
    "        df.to_csv(file_name + \".csv\", index=True)\n",
    "\n",
    "        logging.info(f'Finished and saved {datatype} file')\n",
    "\n",
    "\n",
    "     # These files are not important or are too large\n",
    "    def is_unimportant_file(self, xpt):\n",
    "        lst = [\"DR1IFF\", \"DR2IFF\", \"DSII\", \"AUXAR\", \"PAXHR\", \"PAXMIN\"]\n",
    "        for l in lst:\n",
    "            if re.search(l, xpt):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def read_and_process_xpt(self, url, xpt, datatype):\n",
    "        xpt_url = urljoin(url, xpt)\n",
    "        xpt_df = pd.read_sas(xpt_url)\n",
    "        filename = os.path.basename(xpt)\n",
    "\n",
    "        # For lab data, drop columns ending with \"LC\". These are comment codes.\n",
    "        if datatype == \"laboratory\":\n",
    "            LC_cols = xpt_df.columns.str.endswith('LC')\n",
    "            LC_count = LC_cols.sum()\n",
    "            xpt_df = xpt_df.loc[:, ~LC_cols]\n",
    "            if LC_count > 0:\n",
    "                logging.info(f\"Skipped {LC_count} variables in {filename} because they are large, unimportant comment codes\")\n",
    "        # For examination data, drop Aux files which are large sensor data.\n",
    "        if datatype == \"examination\":\n",
    "            AUX_cols = xpt_df.columns.str.startswith((\"WBX\", \"TYX\"))\n",
    "            AUX_count = AUX_cols.sum()\n",
    "            xpt_df = xpt_df.loc[:, ~AUX_cols]\n",
    "            if AUX_count > 0:\n",
    "                logging.info(f'Skipped {AUX_count} variables in {filename} because they are large, unhelpful sensor data')\n",
    "\n",
    "        if self.id not in xpt_df.columns:\n",
    "            logging.info(f\"{filename} skipped because it's not based on individual participants\")\n",
    "            return pd.DataFrame()\n",
    "        if not xpt_df[self.id].duplicated().any():\n",
    "            xpt_df.set_index(self.id, inplace=True)\n",
    "            return xpt_df\n",
    "        else:\n",
    "            logging.info(f\"{filename} skipped because it has duplicate id values\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def download_data(self):\n",
    "        # Find all data URLs\n",
    "        data_urls = self.find_data_urls()\n",
    "\n",
    "        # Make a folder for the data\n",
    "        os.makedirs(self.title, exist_ok=True)\n",
    "\n",
    "        # Download and convert all data directly\n",
    "        for datatype, data_url in data_urls.items():\n",
    "            self.extract_and_convert_xpt(data_url, datatype)\n",
    "\n",
    "def main():\n",
    "    # Input home page of NHANES data. Default is 2017-2018 data\n",
    "    base_url = f\"https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear={BASE_YEAR}\"\n",
    "    downloader = NHANESDataDownloader(base_url)\n",
    "    downloader.download_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36994779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srnwn\\Documents\\neueFische\\Capstone\\scripts\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
